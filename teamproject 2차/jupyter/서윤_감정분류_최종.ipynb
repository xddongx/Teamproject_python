{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 시작/ 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.backend import tensorflow_backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8000674125313089191, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 3146173644\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 9512908666744007568\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling\n",
    "\n",
    "https://books.google.co.kr/books?id=LTJFDwAAQBAJ&pg=PA134&lpg=PA134&dq=%ED%9D%91%EB%B0%B1%EC%9D%B4%EB%AF%B8%EC%A7%80+%EB%94%A5%EB%9F%AC%EB%8B%9D&source=bl&ots=m58eyFkxQ_&sig=ACfU3U3EGkypUS-G3t6amZqMp0Pvwit8Fg&hl=ko&sa=X&ved=2ahUKEwi2g-i5_drnAhWU7WEKHbApDHwQ6AEwCHoECAkQAQ#v=onepage&q=%ED%9D%91%EB%B0%B1%EC%9D%B4%EB%AF%B8%EC%A7%80%20%EB%94%A5%EB%9F%AC%EB%8B%9D&f=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = './emotion_edit/'\n",
    "afraid_dir= \"./emotion_edit/afraid/\"\n",
    "anger_dir = \"./emotion_edit/anger/\"\n",
    "disgusted_dir = \"./emotion_edit/disgusted/\"\n",
    "happy_dir = \"./emotion_edit/happy/\"\n",
    "neutral_dir = \"./emotion_edit/neutral/\"\n",
    "sad_dir = \"./emotion_edit/sad/\"\n",
    "surprised_dir = \"./emotion_edit/surprised/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os,glob,numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import cv2\n",
    "image_path  = image_dir\n",
    "\n",
    "category = ['afraid','anger','disgusted','happy','neutral','sad','surprised']\n",
    "print(category)\n",
    "\n",
    "n_class = len(category)\n",
    "print(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "image_w =128\n",
    "image_h =128\n",
    "# pixels = image_w * image_h * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, expression  in enumerate(category):\n",
    "    label =[0 for i in range(n_class)] #라벨 초기화 \n",
    "    label[idx]=1\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    \n",
    "    \n",
    "    for i,f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert('L') \n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        X.append(data)\n",
    "        Y.append(label)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y=  np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X,Y)\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape)\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "from keras import models, layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras import optimizers, initializers, regularizers, metrics\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers,initializers,regularizers,metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MAX POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row = 128\n",
    "img_col = 128\n",
    "channel =1\n",
    "n_class = 7\n",
    "batch_size =10\n",
    "n_epoch =16\n",
    "verbose =1\n",
    "validation_split =0.2\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],128,128,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],128,128,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "#1\n",
    "model.add(Conv2D(256,(3,3), padding ='same',input_shape =(img_row,img_col,channel),activation = 'relu'))\n",
    "model.add(Conv2D(64,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(MaxPooling2D(2,2),strides=(2,2))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "#2\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(Conv2D(128,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(MaxPooling2D(2,2),strides=(2,2))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "#3\n",
    "model.add(Conv2D(256,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(Conv2D(256,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(Conv2D(256,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(MaxPooling2D(2,2),stride=(2,2))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "\n",
    "#4\n",
    "\n",
    "model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(MaxPooling2D(2,2),stride=(2,2))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "#5\n",
    "\n",
    "model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "# model.add(MaxPooling2D(2,2),stride=(2,2))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4096,activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(layers.Dense(4096,activation ='relu'))\n",
    "model.add(layers.Dense(2048,activation ='relu'))\n",
    "model.add(layers.Dense(1024,activation ='relu'))\n",
    "model.add(layers.Dense(n_class,activation ='softmax'))\n",
    "# model.add(layers.Dense(7,activation = 'softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers,initializers,regularizers,metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #보류\n",
    "face_model = models.Sequential()\n",
    "face_model.add(prevgg)\n",
    "face_model.add(layers.Flatten())\n",
    "face_model.add(layers.Dense(4096,activation = 'relu'))\n",
    "face_model.add(layers.Dense(2048,activation ='relu'))\n",
    "face_model.add(layers.Dense(1024,activation ='relu'))\n",
    "face_model.add(layers.Dense(7,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'emotion_edit/model/'\n",
    "model_path = model_dir +'vgg16max.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-5), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 사이즈(batch size)는 하나의 batch에 포함되는 샘플의 수.\n",
    "\n",
    "가중치와 편향을 수정하는 간격이라고도 함.\n",
    "\n",
    "배치 사이즈가 너무 크면 학습 속도가 느려지고, 너무 작으면 각 실행값의 편차가 생겨 전체 결과값이 불안정해질 수 있음.\n",
    "\n",
    "따라서 배치 사이즈는 학습 효율에 큰 영향을 주기 때문에 중요함.\n",
    "\n",
    "\n",
    "\n",
    "출처: https://hyeonnii.tistory.com/233 [From the bottom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxmodel = model.fit(X_train,y_train,batch_size = batch_size,epochs = n_epoch,verbose=1,\n",
    "                     validation_split =validation_split,callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('maxmodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./emotion_edit/model/vgg16max_16.model') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,loss = plt.subplots()\n",
    "acc = loss.twinx()\n",
    "\n",
    "loss.plot(model.model['loss'],'b',label ='train loss')\n",
    "loss.plot(model.model['val_loss'],'r',label ='val loss')\n",
    "loss.set_xlabel('epoch')\n",
    "loss.set_ylabel('loss')\n",
    "loss.legend(oc = 'upper right')\n",
    "\n",
    "\n",
    "acc.plot(model.model['accuracy'],'y',label = 'train acc')\n",
    "acc.plot(model.model['val_accuracy'],'g',label = 'val acc')\n",
    "acc.set_ylabel('accuracy')\n",
    "acc.legend(loc ='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss = model.model['val_loss']\n",
    "y_loss = model.model['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = model.model['acc']\n",
    "val_acc = model.model['val_acc']\n",
    "loss = model.model['loss']\n",
    "val_loss = model.model['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제그림 넣고 예측하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os,glob,numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'C:/Users/ICT01_22/Documents/seoy/emotion_edit/facetest'\n",
    "image_w =128\n",
    "image_h =128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = []\n",
    "filenames =[]\n",
    "files = glob.glob(test_dir+ '/' + '*.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,f in enumerate(files):\n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"L\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "#     print(filenames)\n",
    "    print(data)\n",
    "    T.append(data)\n",
    "    print(T)\n",
    "\n",
    "T= np.array(T)\n",
    "\n",
    "model = load_model('./emotion_edit/model/vgg16max_16.model') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = T.reshape(T.shape[0],128,128,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(T)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.astype(float)/255\n",
    "#X_test = X_test.astype(float)/ 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)}) #>> 넘파이 출력옵션 변경하는것! (소수점3자리까지)\n",
    "cnt = 0\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블  # argmax : 함수를 최대로 만들기 위한 x 값 --> 즉 첫번째에서는 3번째만 1이므로 2가 출력됨\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"afraid\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"anger\"\n",
    "    elif pre_ans == 2: pre_ans_str = \"disgusted\"\n",
    "    elif pre_ans == 3: pre_ans_str = \"happy\"\n",
    "    elif pre_ans == 4: pre_ans_str = \"neutral\"\n",
    "    elif pre_ans == 5: pre_ans_str = \"sad\"\n",
    "    \n",
    "    else: pre_ans_str = \"surprised\"\n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"한 기분으로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"한 기분으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"한 기분으로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"한 기분으로 추정됩니다.\")\n",
    "    if i[4] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"한 기분으로 추정됩니다.\")\n",
    "    if i[5] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"한 기분으로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"한 기분으로 추정됩니다.\")    \n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVERAGE POOLING or avg로 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling2D\n",
    "import os,glob,numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "from keras import models, layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras import optimizers, initializers, regularizers, metrics\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers,initializers,regularizers,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = './emotion_edit/'\n",
    "afraid_dir= \"./emotion_edit/afraid/\"\n",
    "anger_dir = \"./emotion_edit/anger/\"\n",
    "disgusted_dir = \"./emotion_edit/disgusted/\"\n",
    "happy_dir = \"./emotion_edit/happy/\"\n",
    "neutral_dir = \"./emotion_edit/neutral/\"\n",
    "sad_dir = \"./emotion_edit/sad/\"\n",
    "surprised_dir = \"./emotion_edit/surprised/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['afraid', 'anger', 'disgusted', 'happy', 'neutral', 'sad', 'surprised']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os,glob,numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import cv2\n",
    "image_path  = image_dir\n",
    "\n",
    "category = ['afraid','anger','disgusted','happy','neutral','sad','surprised']\n",
    "print(category)\n",
    "\n",
    "n_class = len(category)\n",
    "print(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "image_w =128\n",
    "image_h =128\n",
    "# pixels = image_w * image_h * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, expression  in enumerate(category):\n",
    "    label =[0 for i in range(n_class)] #라벨 초기화 \n",
    "    label[idx]=1\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    \n",
    "    \n",
    "    for i,f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert('L') \n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        X.append(data)\n",
    "        Y.append(label)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y=  np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58875, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58875\n",
      "58875\n",
      "44156\n",
      "14719\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X,Y)\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row = 128\n",
    "img_col =128\n",
    "channel =1\n",
    "n_class = 7\n",
    "batch_size =8\n",
    "n_epoch =32\n",
    "validation_split =0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],128,128,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],128,128,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model =Sequential()\n",
    "#1\n",
    "model.add(Conv2D(256,(3,3), padding ='same',input_shape =(img_row,img_col,channel),activation = 'relu'))\n",
    "model.add(Conv2D(64,(3,3),activation ='relu',padding='same'))\n",
    "model.add(AveragePooling2D(2,2))\n",
    "\n",
    "#2\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation ='relu',padding='same'))\n",
    "model.add(Conv2D(128,(3,3),activation ='relu',padding='same'))\n",
    "model.add(AveragePooling2D(2,2))\n",
    "\n",
    "#3\n",
    "model.add(Conv2D(256,(3,3),activation ='relu',padding='same'))\n",
    "model.add(Conv2D(256,(3,3),activation ='relu',padding='same'))\n",
    "model.add(Conv2D(256,(3,3),activation ='relu',padding='same'))\n",
    "model.add(AveragePooling2D(2,2))\n",
    "\n",
    "\n",
    "#4\n",
    "\n",
    "model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "model.add(AveragePooling2D(2,2))\n",
    "\n",
    "\n",
    "#5\n",
    "\n",
    "model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "model.add(Conv2D(512,(3,3),activation ='relu',padding='same'))\n",
    "model.add(AveragePooling2D(2,2))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4096,activation = 'relu'))\n",
    "model.add(layers.Dense(4096,activation ='relu'))\n",
    "model.add(layers.Dense(2048,activation ='relu'))\n",
    "model.add(layers.Dense(1024,activation ='relu'))\n",
    "model.add(layers.Dense(n_class,activation ='softmax'))\n",
    "# model.add(layers.Dense(7,activation = 'softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44156, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dir = 'emotion_edit/model/'\n",
    "model_path = model_dir +'vgg16avg.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-5), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 35324 samples, validate on 8832 samples\n",
      "Epoch 1/32\n",
      "35324/35324 [==============================] - 1124s 32ms/step - loss: 1.9433 - acc: 0.1535 - val_loss: 1.9397 - val_acc: 0.1689\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.93970, saving model to emotion_edit/model/vgg16avg.model\n",
      "Epoch 2/32\n",
      "35324/35324 [==============================] - 1122s 32ms/step - loss: 1.7226 - acc: 0.2974 - val_loss: 1.4393 - val_acc: 0.4368\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.93970 to 1.43934, saving model to emotion_edit/model/vgg16avg.model\n",
      "Epoch 3/32\n",
      "35324/35324 [==============================] - 5822s 165ms/step - loss: 1.2911 - acc: 0.4871 - val_loss: 1.1563 - val_acc: 0.5438\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.43934 to 1.15631, saving model to emotion_edit/model/vgg16avg.model\n",
      "Epoch 4/32\n",
      "35324/35324 [==============================] - 9356s 265ms/step - loss: 1.0566 - acc: 0.5853 - val_loss: 0.9429 - val_acc: 0.6440\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.15631 to 0.94290, saving model to emotion_edit/model/vgg16avg.model\n",
      "Epoch 5/32\n",
      "35324/35324 [==============================] - 9553s 270ms/step - loss: 0.8961 - acc: 0.6535 - val_loss: 0.9243 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.94290 to 0.92433, saving model to emotion_edit/model/vgg16avg.model\n",
      "Epoch 6/32\n",
      "35324/35324 [==============================] - 9269s 262ms/step - loss: 0.7744 - acc: 0.7103 - val_loss: 0.7325 - val_acc: 0.7229\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.92433 to 0.73249, saving model to emotion_edit/model/vgg16avg.model\n",
      "Epoch 7/32\n",
      "35324/35324 [==============================] - 9325s 264ms/step - loss: 0.6938 - acc: 0.7437 - val_loss: 1.0840 - val_acc: 0.6315\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.73249\n",
      "Epoch 8/32\n",
      "35324/35324 [==============================] - 10029s 284ms/step - loss: 0.6432 - acc: 0.7687 - val_loss: 0.7383 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.73249\n",
      "Epoch 9/32\n",
      "35324/35324 [==============================] - 10570s 299ms/step - loss: 0.6043 - acc: 0.7841 - val_loss: 0.7041 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.73249 to 0.70415, saving model to emotion_edit/model/vgg16avg.model\n",
      "Epoch 10/32\n",
      "35324/35324 [==============================] - 10726s 304ms/step - loss: 0.6054 - acc: 0.7938 - val_loss: 0.7766 - val_acc: 0.7065\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.70415\n",
      "Epoch 11/32\n",
      "31968/35324 [==========================>...] - ETA: 16:07 - loss: 0.6457 - acc: 0.7966"
     ]
    }
   ],
   "source": [
    "avgmodel = model.fit(X_train,y_train,validation_split =0.2, batch_size = batch_size , epochs = n_epoch , callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('avgmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
