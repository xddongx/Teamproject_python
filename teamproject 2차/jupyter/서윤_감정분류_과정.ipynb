{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  이미지 감정 인식\n",
    "https://github.com/sunsmiling/facial-emotion-detector\n",
    "\n",
    "\n",
    "2. 영상 실시간 감정 인식\n",
    "https://jusonn.github.io/blog/2018/02/16/real-time-emotion-detection/\n",
    "\n",
    "\n",
    "\n",
    "출처: https://icandooit.tistory.com/95 [i can do \"IT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle \n",
    "\n",
    "- (0=Angry 화남, 1=Disgust역겨움, 2=Fear두려움, 3=Happy행복, 4=Sad슬픔, 5=Surprise놀람, 6=Neutral무표정)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use this dataset in your research work, please cite\n",
    "\n",
    "\"Challenges in Representation Learning: A report on three machine learning\n",
    "contests.\" I Goodfellow, D Erhan, PL Carrier, A Courville, M Mirza, B\n",
    "Hamner, W Cukierski, Y Tang, DH Lee, Y Zhou, C Ramaiah, F Feng, R Li,\n",
    "X Wang, D Athanasakis, J Shawe-Taylor, M Milakov, J Park, R Ionescu,\n",
    "M Popescu, C Grozea, J Bergstra, J Xie, L Romaszko, B Xu, Z Chuang, and\n",
    "Y. Bengio. arXiv 2013.\n",
    "\n",
    "See fer2013.bib for a bibtex entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Pylearn2 Dataset class for accessing the data for the\n",
    "facial expression recognition Kaggle contest for the ICML\n",
    "2013 workshop on representation learning.\n",
    "\"\"\"\n",
    "__authors__ = \"Ian Goodfellow\"\n",
    "__copyright__ = \"Copyright 2013, Universite de Montreal\"\n",
    "__credits__ = [\"Ian Goodfellow\"]\n",
    "__license__ = \"3-clause BSD\"\n",
    "__maintainer__ = \"LISA Lab\"\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pylearn.datasets\n",
    "from pylearn.datasets.dense_design_matrix import DefaultViewConverter\n",
    "from pylearn.datasets.dense_design_matrix import DenseDesignMatrix\n",
    "from pylearn.utils.string_utils import preprocess\n",
    "\n",
    "class EmotionsDataset(DenseDesignMatrix):\n",
    "    \"\"\"\n",
    "    A Pylearn2 Dataset class for accessing the data for the\n",
    "    facial expression recognition Kaggle contest for the ICML\n",
    "    2013 workshop on representation learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, which_set,\n",
    "            base_path = '${C:/Users/ICT01_22/Documents/seoy',\n",
    "            start = None,\n",
    "            stop = None,\n",
    "            preprocessor = None,\n",
    "            fit_preprocessor = False,\n",
    "            axes = ('b', 0, 1, 'c'),\n",
    "            fit_test_preprocessor = False):\n",
    "        \"\"\"\n",
    "        which_set: A string specifying which portion of the dataset\n",
    "            to load. Valid values are 'train' or 'public_test'\n",
    "        base_path: The directory containing the .csv files from kaggle.com.\n",
    "                This directory should be writable; if the .csv files haven't\n",
    "                already been converted to npy, this class will convert them\n",
    "                to save memory the next time they are loaded.\n",
    "        fit_preprocessor: True if the preprocessor is allowed to fit the\n",
    "                   data.\n",
    "        fit_test_preprocessor: If we construct a test set based on this\n",
    "                    dataset, should it be allowed to fit the test set?\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_args = locals()\n",
    "        self.test_args['which_set'] = 'public_test'\n",
    "        self.test_args['fit_preprocessor'] = fit_test_preprocessor\n",
    "        del self.test_args['start']\n",
    "        del self.test_args['stop']\n",
    "        del self.test_args['self']\n",
    "        del self.test_args['__class__']\n",
    "\n",
    "        files = {'train': 'train.csv', 'public_test' : 'test.csv'}\n",
    "\n",
    "        try:\n",
    "            filename = files[which_set]\n",
    "        except KeyError:\n",
    "            raise ValueError(\"Unrecognized dataset name: \" + which_set)\n",
    "\n",
    "        path = base_path + '/' + filename\n",
    "\n",
    "        path = preprocess(path)\n",
    "\n",
    "        X, y = self._load_data(path, which_set == 'train')\n",
    "\n",
    "\n",
    "        if start is not None:\n",
    "            assert which_set != 'test'\n",
    "            assert isinstance(start, int)\n",
    "            assert isinstance(stop, int)\n",
    "            assert start >= 0\n",
    "            assert start < stop\n",
    "            assert stop <= X.shape[0]\n",
    "            X = X[start:stop, :]\n",
    "            if y is not None:\n",
    "                y = y[start:stop, :]\n",
    "\n",
    "        view_converter = DefaultViewConverter(shape=[48,48,1], axes=axes)\n",
    "\n",
    "        if y is None:\n",
    "            y_labels = None\n",
    "        else:\n",
    "            y_labels = 7\n",
    "        super(EmotionsDataset, self).__init__(X=X, y=y, y_labels=y_labels, view_converter=view_converter)\n",
    "\n",
    "        if preprocessor:\n",
    "            preprocessor.apply(self, can_fit=fit_preprocessor)\n",
    "\n",
    "    def adjust_for_viewer(self, X):\n",
    "        return (X - 127.5) / 127.5\n",
    "\n",
    "    def get_test_set(self):\n",
    "        return EmotionsDataset(**self.test_args)\n",
    "\n",
    "    def _load_data(self, path, expect_labels):\n",
    "\n",
    "        assert path.endswith('.csv')\n",
    "\n",
    "        # If a previous call to this method has already converted\n",
    "        # the data to numpy format, load the numpy directly\n",
    "        X_path = path[:-4] + '.X.npy'\n",
    "        Y_path = path[:-4] + '.Y.npy'\n",
    "        if os.path.exists(X_path):\n",
    "            X = np.load(X_path)\n",
    "            if expect_labels:\n",
    "                y = np.load(Y_path)\n",
    "            else:\n",
    "                y = None\n",
    "            return X, y\n",
    "\n",
    "        # Convert the .csv file to numpy\n",
    "        csv_file = open(path, 'r')\n",
    "\n",
    "        reader = csv.reader(csv_file)\n",
    "\n",
    "        # Discard header\n",
    "        row = next(reader)\n",
    "\n",
    "        y_list = []\n",
    "        X_list = []\n",
    "\n",
    "        for row in reader:\n",
    "            if expect_labels:\n",
    "                y_str, X_row_str = (row[0], row[1])\n",
    "                y = int(y_str)\n",
    "                y_list.append([y])\n",
    "            else:\n",
    "                X_row_str = row[1]\n",
    "            X_row_strs = X_row_str.split(' ')\n",
    "            X_row = [float(x) for x in X_row_strs]\n",
    "            X_list.append(X_row)\n",
    "\n",
    "        X = np.asarray(X_list).astype('float32')\n",
    "        if expect_labels:\n",
    "            y = np.asarray(y_list)\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        np.save(X_path, X)\n",
    "        if y is not None:\n",
    "            np.save(Y_path, y)\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv to jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print('Usage: python cv_to_img.py [output_path]')\n",
    "    return -1\n",
    "\n",
    "output_path = sys.argv[1]\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "\n",
    "    os.system('rm -rf {}'.format(output_path))\n",
    "\n",
    "os.system('mkdir {}'.format(output_path))\n",
    "\n",
    "label_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "#data = pd.read_csv('fer2013.csv', delimiter=',')\n",
    "data = np.genfromtxt('fer2013.csv',delimiter=',',dtype=None)\n",
    "\n",
    "labels = data[1:,0].astype(np.int32)\n",
    "image_buffer = data[1:,1]\n",
    "images = np.array([np.fromstring(image, np.uint8, sep=' ') for image in image_buffer])\n",
    "usage = data[1:,2]\n",
    "dataset = zip(labels, images, usage)\n",
    "for i, d in enumerate(dataset):\n",
    "    usage_path = os.path.join(output_path, d[-1])\n",
    "    label_path = os.path.join(usage_path, label_names[d[0]])\n",
    "    img = d[1].reshape((48,48))\n",
    "    img_name = '%08d.jpg' % i\n",
    "    img_path = os.path.join(label_path, img_name)\n",
    "    if not os.path.exists(usage_path):\n",
    "        os.system('mkdir {}'.format(usage_path))\n",
    "    if not os.path.exists(label_path):\n",
    "        os.system('mkdir {}'.format(label_path))\n",
    "    cv2.imwrite(img_path, img)\n",
    "    print ('Write {}'.format(img_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$mkdir fer_images\n",
    "\n",
    "fer_data=pd.read_csv('fer2013/fer2013.csv',delimiter=',')\n",
    "\n",
    "def save_fer_img():\n",
    "\n",
    "for index,row in fer_data.iterrows():\n",
    "pixels=np.asarray(list(row['pixels'].split(' ')),dtype=np.uint8)\n",
    "img=pixels.reshape((48,48))\n",
    "pathname=os.path.join('fer_images',str(index)+'.jpg')\n",
    "cv2.imwrite(pathname,img)\n",
    "print('image saved ias {}'.format(pathname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실시간 영상 처리 \n",
    "\n",
    "##https://www.youtube.com/watch?v=DtBu1u5aBsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow\n",
    "import keras\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers import Conv2D,BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fer2013.csv')\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " print(df.head())\n",
    "    \n",
    "print(df['Usage'].value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,train_y,X_test,test_y = [],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df.iterrows():\n",
    "    val = row['pixels'].split(\".\")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "            X_train.append(np.array(val,'float32'))\n",
    "            train_y.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "            X_test.append(np.array(val,'float32'))\n",
    "            test_y.append(row['emotion'])\n",
    "    except:\n",
    "        print('error occured at index:{index} and ros:{row}')\n",
    "print(f\"X_train sample data:{X_train[0:2]}\")\n",
    "print(f\"train_y sample data:{train_y[0:2]}\")\n",
    "print(f\"X_teste sample data:{X_test[0:2]}\")\n",
    "print(f\"test_y sample data:{test_y[0:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train,'float32')\n",
    "train_y = np.array(train_y , 'float32')\n",
    "X_test = np.array(X_test,'float32')\n",
    "test_y = np.array(test_y,'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  이미지 처리  1\n",
    "-https://github.com/neha01/Realtime-Emotion-Detection/blob/master/emotion_recognition.py#L31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.losses import categorical_crossentropy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fer2013.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN TEST 나누기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test=[],[],[],[]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    val=row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "            X_train.append(np.array(val,'float32'))\n",
    "            y_train.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "            X_test.append(np.array(val,'float32'))\n",
    "            y_test.append(row['emotion'])\n",
    "    except:\n",
    "        print(f\"error occured at index :{index} and row:{row}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "num_labels = 7\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "width, height = 48, 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train,'float32')\n",
    "y_train = np.array(y_train,'float32')\n",
    "X_test = np.array(X_test,'float32')\n",
    "y_test = np.array(y_test,'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'y' one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_classes = num_labels)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes = num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0~1 normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#평균 빼주고  표준편차로 나눠주기\n",
    "\n",
    "X_train -=np.mean(X_train,axis = 0)\n",
    "y_train /=np.std(y_train,axis = 0)\n",
    "\n",
    "X_test -=np.mean(X_test,axis =0)\n",
    "y_test /=np.std(y_train,axis =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],48,48,1)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0],48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape:{X_train.shape}\")\n",
    "print(f\"shape:{y_train.shape}\")\n",
    "\n",
    "\n",
    "print(f\"shape:{X_test.shape}\")\n",
    "print(f\"shape:{y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\n",
    "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "          \n",
    "model.add(MaxPooling2D(pool_size =(2,2),strides =(2,2)))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1024,activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024,activation ='relu'))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_labels, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = categorical_crossentropy,\n",
    "             optimizer =Adam(),\n",
    "             metrics =['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fer = model.fit(X_train,y_train,batch_size = batch_size ,\n",
    "          epochs = epochs , verbose =1, validation_data =(X_test,y_test),shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = fer.fer['loss']\n",
    "# val_loss = fer.fer['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss) +1)\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label = 'Training loss')\n",
    "# plt.plot(epochs,val_loss,'b',label = 'Validation loss')\n",
    "\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()  #그래프 초기화\n",
    "\n",
    "acc = fer.fer['Acc']\n",
    "# val_acc = fer.fer['val_acc']\n",
    "\n",
    "plt.plot(epochs,acc,'bo',label = 'Accuracy')\n",
    "\n",
    "# plt.plot(epochs,val_acc,'b',label = 'val_acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 처리 2 \n",
    "\n",
    "-https://www.kaggle.com/omarensaj/fer-emotion-detection-psd07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense,Conv2D,Activation,MaxPool2D,Flatten,Dropout,BatchNormalization\n",
    "\n",
    "\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fer2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Usage'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train val test 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[['emotion','pixels']][df['Usage'] == 'Training']\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pixels'] =train['pixels'].apply(lambda im:np.fromstring(im,sep=' '))\n",
    "x_train = np.vstack(train['pixels'].values)\n",
    "y_train = np.array(train['emotion'])\n",
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_test_df = df[['emotion','pixels']][df['Usage'] == 'PublicTest']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_test_df['pixels'] = public_test_df['pixels'].apply(lambda im:np.fromstring(im,sep=' '))\n",
    "\n",
    "x_val = np.vstack(public_test_df['pixels'].values)\n",
    "y_val = np.array(public_test_df['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1,48,48,1)\n",
    "x_val = x_val.reshape(-1,48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(0,figsize=(12,6))\n",
    "for i in range(1,13):\n",
    "    plt.subplot(3,4,i)\n",
    "    plt.imshow(x_train[i,:,:,0],cmap='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(width, height, depth):\n",
    "\t\t# initialize the model along with the input shape to be\n",
    "\t\t# \"channels last\" and the channels dimension itself\n",
    "\t\tmodel = Sequential()\n",
    "\t\tchanDim = -1\n",
    "\t\t# CONV => RELU => POOL\n",
    "\t\tmodel.add(Conv2D(64, 3, data_format=\"channels_last\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape=(48, 48, 1)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# (CONV => RELU) * 2 => POOL\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# (CONV => RELU) * 2 => POOL\n",
    "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(1024))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tmodel.add(Dense(7))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size, and image dimensions\n",
    "EPOCHS = 100\n",
    "INIT_LR = 1e-3  #learning rate\n",
    "BS = 32  #BATCH SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModel(width = 48 , height = 48, depth =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = INIT_LR, decay = INIT_LR/EPOCHS)\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = opt,metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range = 25, width_shift_range =0.1,\n",
    "                        height_shift_range = 0.1,shear_range=0.2,zoom_range=0.2,\n",
    "                        horizontal_flip = True,fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(aug.flow(x_train,y_train,batch_size = BS),epochs =EPOCHS,\n",
    "                          shuffle = True,steps_per_epoch = len(x_train)// BS,\n",
    "                          validation_data =(x_val,y_val),\n",
    "                          verbose =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model_json = model.to_json()\n",
    "with open('face_model.json','w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  WEIGHT 저장\n",
    "from keras.models import load_model\n",
    "model.save('fer2013_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델로딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('face_model.json') as json_file:\n",
    "    json_config = json_file.read()\n",
    "new_model = keras.models.model_from_json(json_config)\n",
    "new_model.load_weights('fer2013_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('fer2013_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[['emotion','pixels']][df['Usage']] =='PrivateTest'\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.fit(x_testX_train,y_train,batch_size = batch_size ,\n",
    "          epochs = epochs , verbose =1, validation_data =(X_test,y_test),shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.suptitle('Optimizer : Adam',fontsize =10)\n",
    "plt.ylabel('Loss',fontsize =16)\n",
    "plt.plot(hist.history['loss'],color = 'b',label = 'Training Loss')\n",
    "plt.plot(his.history['val_loss'],color = 'r',label = 'Validation Loss')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.ylabel('Accuracy',fontsize = 16)\n",
    "plt.plot(hist.history['acc'],color = 'b',label = 'Training Accuracy')\n",
    "plt.plot(hist.histpry['val_acc'],color = 'r',label = 'Validation Accuracy')\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 테스트\n",
    "\n",
    "- 경로설정 및 파일 설정\n",
    "-https://wikidocs.net/83\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 생성 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_test_path = 'C:/Users/seoyo/Documents/'\n",
    "os.mkdir('imagetest')  #테스트 폴더 생성\n",
    "\n",
    "# os.mkdir('test_pretraitement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glob는 파일들의 목록을 뽑을 때 사용하는데, 파일의 경로명을 이용해서 입맛대로 요리할 수 있답니다.\n",
    "\n",
    "import glob\n",
    "import cv2 as cv\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "img = 'C:/Users/seoyo/Documents/happyme.jpg'\n",
    "\n",
    "# test_pretraitement = 'test_pretraitement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 불러오기\n",
    "image_path = 'C:/Users/seoyo/Documents/'\n",
    "img = image.load_image(image_path,target_size =(48,48))\n",
    "xhat = image.img_to_array(img)\n",
    "xhat =np.expand_dims(xhat,axis =0)\n",
    "xhat = preprocess_input(xhat)\n",
    "                \n",
    " #임의의 이미지로 예측\n",
    "yhat=model.predict(xhat)\n",
    "\n",
    "\n",
    "#결과확인\n",
    "P = imagenet_utils.decode_predictions(yhat)\n",
    "\n",
    "for(i,(imagenetID,label,prob)) in enumerate(P[0]):\n",
    "    print(\"{}.{}:{:.2f}%\".format(i+1,label,prob*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CascadeClassifier]\n",
    "\n",
    "#https://m.blog.naver.com/PostView.nhn?blogId=samsjang&logNo=220699662173&proxyReferer=https%3A%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이즈 , 색상 변경\n",
    "\n",
    "\n",
    "- vision API 사용한 사이ㅡ : https://bcho.tistory.com/1179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = {}\n",
    "labels_test = {}\n",
    "#CascadeClassifier (다단계 분류)`\n",
    "face_cascade = cv.CascadeClassifier('../input/haarcascade/haarcascade_frontalface_default.xml')\n",
    "i = 0\n",
    "for img in glob.glob(image_test_path+'/*.jpg'):\n",
    "    image = cv.imread(img)\n",
    "    name  = img.split('/')[-1]\n",
    "    \n",
    "    gray_image =cv.cvtColor(image,cv.COLOR_BGR2GRAY) #c0nvert to grey\n",
    "    height,width = image.shape[:2]\n",
    "    faces =face_cascade.detectMultiScale(gray_image,1.3,1)\n",
    "    if isinstance(faces,tuple):\n",
    "        resized_image=cv.resize(gray_image(48,48))\n",
    "        cv.imwrite(image_test_path+'/'+name,resized_image)\n",
    "#         print \n",
    "    elif isinstance(faces,np.ndarray):\n",
    "        for(x,y,w,h) in faces:\n",
    "            if w*h <(height * width)/3 :\n",
    "                resized_image =cv.resize(gray_image,(48,48))\n",
    "                cv.imwrite(image_test_path+'/'+name,resized_image)\n",
    "            else:\n",
    "                #cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                roi_gray = gray_image[y:y+h,x:x+w]\n",
    "                #print(len(roi_gray))\n",
    "                resized_image = cv.resize(roi_gray,(48,48))\n",
    "                cv.imwrite(image_test_path+'/'+name.resized_image)\n",
    "    image = resized_image.astype('float')/255.0\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image,axis =0)\n",
    "    data_test[name] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data_test['happyme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predict ={}\n",
    "for key,value in data_test.items():\n",
    "    predict = model.predict(value)\n",
    "    idx = np.argmax(predict)\n",
    "    data_predict[key] =idx\n",
    "    \n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame(list(data_predict.items()),\n",
    "                         columns =['Image','Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fd = io.open(imagefile,'rb')\n",
    "\n",
    "# image = Image.open(fd)\n",
    "\n",
    " \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # display original image\n",
    "\n",
    "# print \"Original image\"\n",
    "\n",
    "# plt.imshow(image)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 출처: https://bcho.tistory.com/1179 [조대협의 블로그]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 불러오기\n",
    "image_path = ''\n",
    "img = image.load_imag(imga_path,target_size =(48,48))\n",
    "xhat = image.img_to_array(img)\n",
    "xhat =np.expand_dims(xhat,axis =0)\n",
    "xhat = preprocess_input(xhat)\n",
    "                \n",
    " #임의의 이미지로 예측\n",
    "yhat=model.predict(xhat)\n",
    "\n",
    "\n",
    "#결과확인\n",
    "P = imagenet_utils.decode_predictions(yhat)\n",
    "\n",
    "for(i,(imagenetID,label,prob)) in enumerate(P[0]):\n",
    "    print(\"{}.{}:{:.2f}%\".format(i+1,label,prob*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)}) #>> 넘파이 출력옵션 변경하는것! (소수점3자리까지)\n",
    "cnt = 0\n",
    "for i in prediction2:\n",
    "    pre_ans = i.argmax()  # 예측 레이블  # argmax : 함수를 최대로 만들기 위한 x 값 --> 즉 첫번째에서는 3번째만 1이므로 2가 출력됨\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"캔\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"플라스틱\"\n",
    "    elif pre_ans == 2: pre_ans_str = \"유리\"\n",
    "    else: pre_ans_str = \"스티로품\"\n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDEF\n",
    "\n",
    "- https://www.slideshare.net/WonjuSeo/facial-emotion-recognition-with-keras\n",
    "\n",
    "- https://github.com/Wonjuseo/Facial_Expression\n",
    "- https://github.com/Wonjuseo/Facial_Expression/blob/master/FER/data_load.py\n",
    "\n",
    "When I tried lots of tests,\n",
    "1. Smooth labels improved greatly accuracy.(rather than one-hot labels - argmax(emotions))\n",
    "2. When parameteric ReLU was used, AveragePooling showed better results than when maxpooling was used\n",
    "3. To avert over-fitting problems, I used \n",
    "  1. Data augmentation (Image augmentation)\n",
    "  2. Add Gausssian noise\n",
    "  3. Global Average Pooling layer, rather than Flatten\n",
    "  4. Dropout layers with 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 convert\n",
    "\n",
    "- https://antilibrary.org/1980\n",
    "- https://keras.io/ko/getting-started/sequential-model-guide/\n",
    "- https://github.com/Rakshith-2905/Face-Recognition/blob/master/README.md\n",
    "\n",
    "\n",
    "[중국인거]\n",
    "- https://github.com/543877815/KDEF/blob/master/pre-processing.ipynb\n",
    "\n",
    "\n",
    "[real time / fer2013& kdef]\n",
    "-https://github.com/umit-ozturk/Final-Project-with-CNN/tree/master/src\n",
    "\n",
    "- \n",
    "- afraid\n",
    "- anger\n",
    "- disgusted\n",
    "- happy\n",
    "- neutral\n",
    "- sad\n",
    "- surprised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 이미지 불러오기\n",
    "2. 이미지 흑백지정, 사이즈 지정 / 변환\n",
    "3. train val test 나누기\n",
    "4. grid search \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np , pandas as pd\n",
    "import os \n",
    "from scipy import misc\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense,Dropout,Flatten,MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(os.listdir(afraid_dir))\n",
    "# len(os.listdir(anger_dir))\n",
    "len(os.listdir(disgusted_dir))\n",
    "# len(os.listdir(happy_dir))\n",
    "# len(os.listdir(neutral_dir))\n",
    "# len(os.listdir(sad_dir))\n",
    "# len(os.listdir(surprised_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = list(os.listdir('./emotion/'))\n",
    "afraid= list(os.listdir(\"./emotion/afraid/\"))\n",
    "anger = list(os.listdir(\"./emotion/anger/\"))\n",
    "disgusted = list(os.listdir(\"./emotion/disgusted/\"))\n",
    "happy = list(os.listdir(\"./emotion/happy/\"))\n",
    "neutral = list(os.listdir(\"./emotion/neutral\"))\n",
    "sad = list(os.listdir(\"./emotion/sad\"))\n",
    "surprised = list(os.listdir(\"./emotion/surprised\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_test = {}\n",
    "labels_test = {}\n",
    "#CascadeClassifier (다단계 분류)`\n",
    "face_cascade = cv.CascadeClassifier('../input/haarcascade/haarcascade_frontalface_default.xml')\n",
    "i = 0\n",
    "for img in glob.glob(image_test_path+'/*.jpg'):\n",
    "    image = cv.imread(img)\n",
    "    name  = img.split('/')[-1]\n",
    "    \n",
    "    gray_image =cv.cvtColor(image,cv.COLOR_BGR2GRAY) #c0nvert to grey\n",
    "    height,width = image.shape[:2]\n",
    "    faces =face_cascade.detecMultiScale(gray_image,1.3,1)\n",
    "    if isinstance(faces,tuple):\n",
    "        resized_image=cv.resize(gray_image(48,48))\n",
    "        cv.imwrite(image_test_path+'/'+name,resized_image)\n",
    "        #print \n",
    "    elif isinstance(faces,np.ndarray):\n",
    "        for(x,y,w,h) in faces:\n",
    "            if w*h <(height * width)/3 :\n",
    "                resized_image =cv.resize(gray_image,(48,48))\n",
    "                cv.imwrite(image_test_path+'/'+name,resized_image)\n",
    "            else:\n",
    "                #cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                roi_gray = gray_image[y:y+h,x:x+w]\n",
    "                #print(len(roi_gray))\n",
    "                resized_image = cv.resize(roi_gray,(48,48))\n",
    "                cv.imwrite(image_test_path+'/'+name.resized_image)\n",
    "    image = resized_image.astype('float')/255.0\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image,axis =0)\n",
    "    data_test[name] = image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "afraid_dir = './emotion/afraid'\n",
    "\n",
    "\n",
    "for img in glob.glob(afraid_dir+'/*.JPG'):\n",
    "    image =cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "plt.show(1,2)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './emotion/'\n",
    "afraid_dir= \"./emotion/afraid/\"\n",
    "anger_dir = \"./emotion/anger/\"\n",
    "disgusted_dir = \"./emotion/disgusted/\"\n",
    "happy_dir = \"./emotion/happy/\"\n",
    "neutral_dir = \"./emotion/neutral\"\n",
    "sad_dir = \"./emotion/sad\"\n",
    "surprised_dir = \"./emotion/surprised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(afraid_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일명/사이즈/흑백 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense,Dropout,Flatten,MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np , pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "afraid = cv2.imread('./emotion/afraid/AF01AFFL.JPG')\n",
    "print(afraid.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### afraid_다시하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_af = list(os.listdir('emotion/afraid/'))\n",
    "\n",
    "title_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,re in enumerate(title_af):\n",
    "    afraid = cv2.imread('emotion/afraid/{}'.format(re))\n",
    "    afraid_rs =cv2.resize(afraid,(256,256))\n",
    "    afraid_ =cv2.cvtColor(afraid_rs,cv2.COLOR_BGR2GRAY)\n",
    "    print('afraid_resize.shape ={0}'.format(afraid_rs.shape))\n",
    "    \n",
    "    cv2.imwrite('./emotion_edit/afraid/afraid{}.jpg'.format(i),afraid_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_an = os.listdir('emotion/anger/')\n",
    "\n",
    "# print(title_an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,re in enumerate(title_an):\n",
    "    anger = cv2.imread('emotion/anger/{}'.format(re))\n",
    "    anger_rs = cv2.resize(anger,(256,256))\n",
    "    anger_ =cv2.cvtColor(anger_rs,cv2.COLOR_BGR2GRAY)\n",
    "    print('anger_rs.shape ={0}'.format(anger_rs.shape))\n",
    "    cv2.imwrite('./emotion_edit/anger/anger{}.jpg'.format(i),anger_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disgusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_di = os.listdir('emotion/disgusted/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,re in enumerate(title_di):\n",
    "    disgusted = cv2.imread('emotion/disgusted/{}'.format(re))\n",
    "    disgusted_rs =cv2.resize(disgusted,(256,256))\n",
    "    disgusted_ =cv2.cvtColor(disgusted_rs,cv2.COLOR_BGR2GRAY)\n",
    "    print('disgusted_rs.shape ={0}'.format(disgusted_rs.shape))\n",
    "    cv2.imwrite('./emotion_edit/disgusted/disgusted{}.jpg'.format(i),disgusted_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ha= os.listdir('emotion/happy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,re in enumerate(title_ha):\n",
    "    happy = cv2.imread('emotion/happy/{}'.format(re))\n",
    "    happy_rs=cv2.resize(happy,(256,256))\n",
    "    happy_ =cv2.cvtColor(happy_rs,cv2.COLOR_BGR2GRAY)\n",
    "    print('happy_rs.shape={0}'.format(happy_rs.shape))\n",
    "    cv2.imwrite('./emotion_edit/happy/happy{}.jpg'.format(i),happy_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ne = os.listdir('emotion/neutral/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,re in enumerate(title_ne):\n",
    "    neutral=cv2.imread('emotion/neutral/{}'.format(re))\n",
    "    neutral_rs = cv2.resize(neutral,(256,256))\n",
    "    neutral_ =cv2.cvtColor(neutral_rs,cv2.COLOR_BGR2GRAY)\n",
    "    print('neutral_rs.shape ={0}'.format(neutral_rs.shape))\n",
    "    cv2.imwrite('./emotion_edit/neutral/neutral{}.jpg'.format(i),neutral_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_sa = os.listdir('emotion/sad/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,re in enumerate(title_sa):\n",
    "    sad = cv2.imread('emotion/sad/{}'.format(re))\n",
    "    sad_rs = cv2.resize(sad,(256,256))\n",
    "    sad_ =cv2.cvtColor(sad_rs,cv2.COLOR_BGR2GRAY)\n",
    "    print('sad_rs.shape={0}'.format(sad_rs.shape))\n",
    "    cv2.imwrite('./emotion_edit/sad/sad{}.jpg'.format(i),sad_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_su = os.listdir('emotion/surprised/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,re in enumerate(title_su):\n",
    "    surprised=cv2.imread('emotion/surprised/{0}'.format(re))\n",
    "    surprised_rs = cv2.resize(surprised,(256,256))\n",
    "    surprised_ =cv2.cvtColor(surprised_rs,cv2.COLOR_BGR2GRAY)\n",
    "    print('surprised_rs.shape ={0}'.format(surprised_rs.shape))\n",
    "    cv2.imwrite('./emotion_edit/surprised/surprised{}.jpg'.format(i),surprised_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = './emotion_edit/'\n",
    "afraid_dir= \"./emotion_edit/afraid/\"\n",
    "anger_dir = \"./emotion_edit/anger/\"\n",
    "disgusted_dir = \"./emotion_edit/disgusted/\"\n",
    "happy_dir = \"./emotion_edit/happy/\"\n",
    "neutral_dir = \"./emotion_edit/neutral/\"\n",
    "sad_dir = \"./emotion_edit/sad/\"\n",
    "surprised_dir = \"./emotion_edit/surprised/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['afraid' 'anger' 'disgusted' 'happy' 'model' 'neutral' 'numpy_data' 'sad'\n",
      " 'surprised']\n",
      "afraid length : 8435\n",
      "anger length : 8399\n",
      "disgusted length : 8365\n",
      "happy length : 8398\n",
      "model length : 0\n",
      "neutral length : 8475\n",
      "numpy_data length : 0\n",
      "sad length : 8428\n",
      "surprised length : 8375\n"
     ]
    }
   ],
   "source": [
    "import os,glob,numpy as np\n",
    "from numpy import array\n",
    "category = array(os.listdir(image_dir))\n",
    "print(category)\n",
    "\n",
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir +'/' +expression\n",
    "    files = glob.glob(image_path +'/*.jpg')\n",
    "    print(expression,'length :',len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array,load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rescale=1./255,rotation_range = 25, width_shift_range =0.1,\n",
    "                        height_shift_range = 0.1,shear_range=0.2,zoom_range=0.3,\n",
    "                        horizontal_flip = True,fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### afraid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    if expression == 'afraid':\n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,)+x.shape)\n",
    "            i =0\n",
    "            for batch in aug.flow(x,batch_size =1,save_to_dir = afraid_dir,\n",
    "                                  save_prefix = 'afraid_gen',save_format='jpg'):\n",
    "                i+=1\n",
    "                if i > 20:\n",
    "                    break\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "from numpy import array\n",
    "category = array(os.listdir(image_dir))\n",
    "print(category)\n",
    "\n",
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir +'/' +expression\n",
    "    files = glob.glob(image_path +'/*.jpg')\n",
    "    print(expression,'length :',len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    if expression == 'anger':\n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,)+x.shape)\n",
    "            i =0\n",
    "            for batch in aug.flow(x,batch_size =1,save_to_dir = anger_dir,\n",
    "                                  save_prefix = 'anger_gen',save_format='jpg'):\n",
    "                i+=1\n",
    "                if i > 20:\n",
    "                    break\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "from numpy import array\n",
    "category = array(os.listdir(image_dir))\n",
    "print(category)\n",
    "\n",
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir +'/' +expression\n",
    "    files = glob.glob(image_path +'/*.jpg')\n",
    "    print(expression,'length :',len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disgusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    if expression == 'disgusted':\n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,)+x.shape)\n",
    "            i =0\n",
    "            for batch in aug.flow(x,batch_size =1,save_to_dir = disgusted_dir,\n",
    "                                  save_prefix = 'disgusted_gen',save_format='jpg'):\n",
    "                i+=1\n",
    "                if i > 20:\n",
    "                    break\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "from numpy import array\n",
    "category = array(os.listdir(image_dir))\n",
    "print(category)\n",
    "\n",
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir +'/' +expression\n",
    "    files = glob.glob(image_path +'/*.jpg')\n",
    "    print(expression,'length :',len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    if expression == 'happy':\n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,)+x.shape)\n",
    "            i =0\n",
    "            for batch in aug.flow(x,batch_size =1,save_to_dir = happy_dir,\n",
    "                                  save_prefix = 'happy_gen',save_format='jpg'):\n",
    "                i+=1\n",
    "                if i > 20:\n",
    "                    break\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "from numpy import array\n",
    "category = array(os.listdir(image_dir))\n",
    "print(category)\n",
    "\n",
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir +'/' +expression\n",
    "    files = glob.glob(image_path +'/*.jpg')\n",
    "    print(expression,'length :',len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    if expression == 'neutral':\n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,)+x.shape)\n",
    "            i =0\n",
    "            for batch in aug.flow(x,batch_size =1,save_to_dir = neutral_dir,\n",
    "                                  save_prefix = 'neutral_gen',save_format='jpg'):\n",
    "                i+=1\n",
    "                if i > 20:\n",
    "                    break\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "from numpy import array\n",
    "category = array(os.listdir(image_dir))\n",
    "print(category)\n",
    "\n",
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir +'/' +expression\n",
    "    files = glob.glob(image_path +'/*.jpg')\n",
    "    print(expression,'length :',len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    if expression == 'sad':\n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,)+x.shape)\n",
    "            i =0\n",
    "            for batch in aug.flow(x,batch_size =1,save_to_dir = sad_dir,\n",
    "                                  save_prefix = 'sad_gen',save_format='jpg'):\n",
    "                i+=1\n",
    "                if i > 20:\n",
    "                    break\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "from numpy import array\n",
    "category = array(os.listdir(image_dir))\n",
    "print(category)\n",
    "\n",
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir +'/' +expression\n",
    "    files = glob.glob(image_path +'/*.jpg')\n",
    "    print(expression,'length :',len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    if expression == 'surprised':\n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,)+x.shape)\n",
    "            i =0\n",
    "            for batch in aug.flow(x,batch_size =1,save_to_dir = surprised_dir,\n",
    "                                  save_prefix = 'surprised_gen',save_format='jpg'):\n",
    "                i+=1\n",
    "                if i > 20:\n",
    "                    break\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "from numpy import array\n",
    "category = array(os.listdir(image_dir))\n",
    "print(category)\n",
    "\n",
    "for idx,expression in enumerate(category):\n",
    "    image_path = image_dir +'/' +expression\n",
    "    files = glob.glob(image_path +'/*.jpg')\n",
    "    print(expression,'length :',len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 배열화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = './emotion_edit/'\n",
    "afraid_dir= \"./emotion_edit/afraid/\"\n",
    "anger_dir = \"./emotion_edit/anger/\"\n",
    "disgusted_dir = \"./emotion_edit/disgusted/\"\n",
    "happy_dir = \"./emotion_edit/happy/\"\n",
    "neutral_dir = \"./emotion_edit/neutral/\"\n",
    "sad_dir = \"./emotion_edit/sad/\"\n",
    "surprised_dir = \"./emotion_edit/surprised/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['afraid', 'anger', 'disgusted', 'happy', 'neutral', 'sad', 'surprised']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os,glob,numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import cv2\n",
    "image_path  = image_dir\n",
    "\n",
    "category = ['afraid','anger','disgusted','happy','neutral','sad','surprised']\n",
    "print(category)\n",
    "\n",
    "n_class = len(category)\n",
    "print(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "a = cv2.imread('./emotion_edit/neutral/neutral_gen_0_108.JPG')\n",
    "print(a.shape)\n",
    "# cv2.imshow('a',a)\n",
    "\n",
    "# b = a.convert('L')\n",
    "# print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "image_w =128\n",
    "image_h =128\n",
    "# pixels = image_w * image_h * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, expression  in enumerate(category):\n",
    "    label =[0 for i in range(n_class)] #라벨 초기화 \n",
    "    label[idx]=1\n",
    "    image_path = image_dir+'/'+ expression\n",
    "    files = glob.glob(image_path+'/*.jpg')\n",
    "    \n",
    "    \n",
    "    for i,f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert('RGB') \n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        X.append(data)\n",
    "        Y.append(label)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y=  np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58875, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train val test 나누기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58875\n",
      "58875\n",
      "44156\n",
      "14719\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X,Y)\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xy= (X_train, X_test, y_train,y_test)\n",
    "np.save ('./emotion_edit/numpy_data/multi.npy',xy)\n",
    "\n",
    "\n",
    "\n",
    "#pickle.dump(d,open)\n",
    "#X_train, X_test, y_train, y_test = np.load('./emotion_edit/numpy_data/multi.npy',allow_pickle =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape)\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크기가 커서 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cv2.imread('./emotion_edit/neutral/neutral_gen_0_108.JPG')\n",
    "print(a.shape)\n",
    "plt.imshow(a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.applications import VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "from keras import models, layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras import optimizers, initializers, regularizers, metrics\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "channel =1\n",
    "img_row=128\n",
    "img_col=128\n",
    "batch_size =32\n",
    "n_epoch =40\n",
    "n_class = 7\n",
    "verbose =1\n",
    "validation_split =0.2\n",
    "learning_rate = 0.01\n",
    "#optimizer = RMSprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model =Sequential()\n",
    "model.add(Conv2D(256,kernel =3, padding ='same',\n",
    "                 input_shape =(img_row,img_col,channel),activation = 'relu'))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,kernel =3, padding ='same',activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "\n",
    "model.add(Conv2D(128,kernel =3, padding = 'same',activation = 'relu'))\n",
    "model.add(Conv2D(128,kernel = 3, padding = 'same',activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(256,kernel =3, padding = 'same',activation = 'relu'))\n",
    "model.add(Conv2D(256,kernel = 3, padding = 'same',activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],128,128,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],128,128,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44156, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers,initializers,regularizers,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prevgg = VGG16(weights='imagenet',include_top = False, input_shape = (128,128,3))\n",
    "prevgg.trainable = False\n",
    "\n",
    "prevgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model = models.Sequential()\n",
    "face_model.add(prevgg)\n",
    "face_model.add(layers.Flatten())\n",
    "face_model.add(layers.Dense(4096,activation = 'relu'))\n",
    "face_model.add(layers.Dense(2048,activation ='relu'))\n",
    "face_model.add(layers.Dense(1024,activation ='relu'))\n",
    "face_model.add(layers.Dense(7,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 58,769,223\n",
      "Trainable params: 44,054,535\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "face_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'emotion_edit/model'\n",
    "model_path = model_dir +'vgg16.model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "face_model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-5), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/31\n",
      "44156/44156 [==============================] - 3964s 90ms/step - loss: 1.6995 - acc: 0.3274\n",
      "Epoch 2/31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_22\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22080/44156 [==============>...............] - ETA: 33:01 - loss: 1.4292 - acc: 0.4540"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c538e6e41f20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = face_model.fit(X_train,y_train,batch_size = 32,epochs =31,\n\u001b[1;32m----> 2\u001b[1;33m                          callbacks =[checkpoint,early_stopping])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = face_model.fit(X_train,y_train,batch_size = 32,epochs =31,validation_split=0.2,\n",
    "                         callbacks =[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
