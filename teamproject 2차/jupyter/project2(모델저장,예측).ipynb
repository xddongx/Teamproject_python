{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wikidocs.net/44249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re                              # 정규표현식을 지원한다 \n",
    "from konlpy.tag import Okt             # 한국어 처리 패키지 \n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # 토큰화(나눠준다고생각하자)\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # 샘플의 길이 동일하게(패딩), 길이가 다른 경우 0을 넣어서 맞춰준다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 확인, train,test 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_excel(\"C:/Users/ICT01_17/Documents/project2/wordtrainfinal.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1395</td>\n",
       "      <td>우기다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>586</td>\n",
       "      <td>두렵고 싫었다</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>832</td>\n",
       "      <td>간절하다</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>958</td>\n",
       "      <td>분하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>1335</td>\n",
       "      <td>벅차다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>43</td>\n",
       "      <td>화내면</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>78</td>\n",
       "      <td>이쁜 선물 받았습니다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1455</td>\n",
       "      <td>짜증나다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>231</td>\n",
       "      <td>피곤</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>818</td>\n",
       "      <td>고맙다 만족스럽다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1499 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     document  label\n",
       "1393  1395          우기다      0\n",
       "126    586      두렵고 싫었다      4\n",
       "830    832         간절하다      3\n",
       "956    958          분하다      0\n",
       "1333  1335          벅차다      1\n",
       "...    ...          ...    ...\n",
       "663     43          화내면      0\n",
       "449     78  이쁜 선물 받았습니다      1\n",
       "1453  1455         짜증나다      0\n",
       "608    231           피곤      3\n",
       "816    818    고맙다 만족스럽다      1\n",
       "\n",
       "[1499 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행 무작위로 순서바꾸자.\n",
    "data=data.sample(frac=1)  # >> 모든행을 임의의 순서로 반환한다.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=data.iloc[:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.iloc[200:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특수문자 제거 (한글만 남기고 제거하기위해서 정규 표현식을 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "# >> 한글과 공백을 제외하고 모두 제거한다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화와 불용어(조사나 접속사 같은 것) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','을',\n",
    "             '를','으로','자','에','와','한','하다','로','이다']\n",
    "\n",
    "# >> 토큰화를 위한 형태소 분석기는 KoNLPy 의 Okt 를 사용한다\n",
    "# >> KoNLPy : 띄어쓰기,알고리즘 정규화를 이용해서 맞춤법 틀린 문장 어느정도 고쳐주면서 형태소 분석과 품사를 태깅\n",
    "\n",
    "okt = Okt() # KoNLPy 에서 제공하는 형태소 분석기이다.(영어는 띄어쓰기 기준으로 토큰화하지만 한국어는 주로 형태소로 나눈다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True, norm=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train) # >>형태소 토큰화가 진행되었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "X_test=[]\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True, norm=True)\n",
    "    temp_X = [word for word in temp_X if not word in stopwords]\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정수 인코딩(X_train, X_test 에 대해서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "tokenizer = Tokenizer(num_words=max_words) # 상위 2000개 단어만 보존\n",
    "\n",
    "tokenizer.fit_on_texts(X_train) # 단어 인덱스를 구축 \n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train) # 문자열을 정수 인덱스의 리스트로 변환\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "#print(X_train) # 단어 대신 단어에 대한 인덱스 부여 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 길이 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글자 최대 길이 : 10\n"
     ]
    }
   ],
   "source": [
    "print('글자 최대 길이 :',max(len(l) for l in X_train))\n",
    "\n",
    "# 모델이 처리할 수 있도록 X_train, X_test 의 모든 샘플의 길이를 동일하게 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0  36  36]\n",
      " [  0   0   0 ...   0   0 365]\n",
      " [  0   0   0 ...   0 176 177]\n",
      " ...\n",
      " [  0   0   0 ...   0   0  20]\n",
      " [  0   0   0 ...   0   0 164]\n",
      " [  0   0   0 ...   0  34 152]]\n"
     ]
    }
   ],
   "source": [
    "max_len =8 # 길이를 10으로 정했다\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "print(X_train) # 패딩 된것을 확인할수있다( 없는 부분은 0으로 채움)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 레이블 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "# print(y_train)\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM로 감성 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "\n",
    "#model.add(Embedding(max_words,100))  # 임베딩 층(이것도 인공 신경망의 층 중 하나) \n",
    "                        #embedding(number of samples, input_length) >> 벡터의 차원은 100으로\n",
    "\n",
    "#model.add(LSTM(128)) # 감정 분류를 위해서 LSTM 사용\n",
    "\n",
    "#model.add(Dense(8, activation='relu'))\n",
    "#model.add(Dense(1, activation='relu'))\n",
    "#model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "#history = model.fit(X_train, y_train, epochs=10, batch_size=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Embedding(max_words,100))  \n",
    "#model.add(LSTM(128)) \n",
    "#model.add(Dense(8, activation='relu'))\n",
    "#model.add(Dense(1, activation='relu'))\n",
    "#model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "#history = model.fit(X_train, y_train, epochs=10, batch_size=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\n",
    "#print(model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화(scaling)\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#sc=MinMaxScaler()\n",
    "#train_sc = sc.fit_transform(X_train)\n",
    "#test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 1039 samples, validate on 260 samples\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "1039/1039 [==============================] - 1s 550us/sample - loss: 4.6325 - acc: 0.2570 - val_loss: 2.2251 - val_acc: 0.3269\n",
      "Epoch 2/50\n",
      "1039/1039 [==============================] - 0s 167us/sample - loss: 2.3988 - acc: 0.2685 - val_loss: 2.1077 - val_acc: 0.3269\n",
      "Epoch 3/50\n",
      "1039/1039 [==============================] - 0s 157us/sample - loss: 2.2554 - acc: 0.2685 - val_loss: 2.0547 - val_acc: 0.3269\n",
      "Epoch 4/50\n",
      "1039/1039 [==============================] - 0s 161us/sample - loss: 2.0387 - acc: 0.2685 - val_loss: 1.9750 - val_acc: 0.3269\n",
      "Epoch 5/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 1.7440 - acc: 0.2705 - val_loss: 1.8230 - val_acc: 0.3269\n",
      "Epoch 6/50\n",
      "1039/1039 [==============================] - 0s 175us/sample - loss: 1.2396 - acc: 0.2724 - val_loss: 1.7497 - val_acc: 0.3269\n",
      "Epoch 7/50\n",
      "1039/1039 [==============================] - 0s 161us/sample - loss: 0.8108 - acc: 0.2868 - val_loss: 1.6994 - val_acc: 0.3269\n",
      "Epoch 8/50\n",
      "1039/1039 [==============================] - 0s 157us/sample - loss: 0.5263 - acc: 0.3195 - val_loss: 1.7143 - val_acc: 0.3462\n",
      "Epoch 9/50\n",
      "1039/1039 [==============================] - 0s 158us/sample - loss: 0.3712 - acc: 0.3609 - val_loss: 1.7342 - val_acc: 0.3500\n",
      "Epoch 10/50\n",
      "1039/1039 [==============================] - 0s 156us/sample - loss: 0.2774 - acc: 0.3821 - val_loss: 1.7520 - val_acc: 0.3615\n",
      "Epoch 11/50\n",
      "1039/1039 [==============================] - 0s 159us/sample - loss: 0.2206 - acc: 0.4090 - val_loss: 1.7734 - val_acc: 0.3654\n",
      "Epoch 12/50\n",
      "1039/1039 [==============================] - 0s 157us/sample - loss: 0.1820 - acc: 0.4620 - val_loss: 1.8046 - val_acc: 0.3692\n",
      "Epoch 13/50\n",
      "1039/1039 [==============================] - 0s 160us/sample - loss: 0.1619 - acc: 0.4745 - val_loss: 1.8148 - val_acc: 0.3808\n",
      "Epoch 14/50\n",
      "1039/1039 [==============================] - 0s 166us/sample - loss: 0.1504 - acc: 0.4832 - val_loss: 1.8530 - val_acc: 0.3692\n",
      "Epoch 15/50\n",
      "1039/1039 [==============================] - 0s 170us/sample - loss: 0.1393 - acc: 0.4860 - val_loss: 1.8980 - val_acc: 0.3692\n",
      "Epoch 16/50\n",
      "1039/1039 [==============================] - 0s 174us/sample - loss: 0.1282 - acc: 0.4851 - val_loss: 1.8723 - val_acc: 0.3769\n",
      "Epoch 17/50\n",
      "1039/1039 [==============================] - 0s 175us/sample - loss: 0.1245 - acc: 0.4860 - val_loss: 1.8973 - val_acc: 0.3769\n",
      "Epoch 18/50\n",
      "1039/1039 [==============================] - 0s 174us/sample - loss: 0.1195 - acc: 0.4870 - val_loss: 1.9023 - val_acc: 0.3769\n",
      "Epoch 19/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.1190 - acc: 0.4870 - val_loss: 1.9146 - val_acc: 0.3769\n",
      "Epoch 20/50\n",
      "1039/1039 [==============================] - 0s 160us/sample - loss: 0.1150 - acc: 0.4880 - val_loss: 1.9362 - val_acc: 0.3769\n",
      "Epoch 21/50\n",
      "1039/1039 [==============================] - 0s 169us/sample - loss: 0.1153 - acc: 0.4880 - val_loss: 1.9388 - val_acc: 0.3731\n",
      "Epoch 22/50\n",
      "1039/1039 [==============================] - 0s 163us/sample - loss: 0.1149 - acc: 0.4889 - val_loss: 1.9568 - val_acc: 0.3654\n",
      "Epoch 23/50\n",
      "1039/1039 [==============================] - 0s 161us/sample - loss: 0.1103 - acc: 0.4899 - val_loss: 1.9492 - val_acc: 0.3692\n",
      "Epoch 24/50\n",
      "1039/1039 [==============================] - 0s 159us/sample - loss: 0.1069 - acc: 0.4899 - val_loss: 1.9698 - val_acc: 0.3654\n",
      "Epoch 25/50\n",
      "1039/1039 [==============================] - 0s 159us/sample - loss: 0.1056 - acc: 0.4889 - val_loss: 1.9637 - val_acc: 0.3692\n",
      "Epoch 26/50\n",
      "1039/1039 [==============================] - 0s 158us/sample - loss: 0.1057 - acc: 0.4937 - val_loss: 1.9700 - val_acc: 0.3731\n",
      "Epoch 27/50\n",
      "1039/1039 [==============================] - 0s 161us/sample - loss: 0.1118 - acc: 0.4918 - val_loss: 1.9851 - val_acc: 0.3731\n",
      "Epoch 28/50\n",
      "1039/1039 [==============================] - 0s 159us/sample - loss: 0.1143 - acc: 0.4909 - val_loss: 1.9979 - val_acc: 0.3692\n",
      "Epoch 29/50\n",
      "1039/1039 [==============================] - 0s 160us/sample - loss: 0.1058 - acc: 0.4947 - val_loss: 1.9751 - val_acc: 0.3731\n",
      "Epoch 30/50\n",
      "1039/1039 [==============================] - 0s 174us/sample - loss: 0.1070 - acc: 0.4928 - val_loss: 2.0336 - val_acc: 0.3692\n",
      "Epoch 31/50\n",
      "1039/1039 [==============================] - 0s 178us/sample - loss: 0.1025 - acc: 0.4937 - val_loss: 1.9909 - val_acc: 0.3769\n",
      "Epoch 32/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 0.1010 - acc: 0.4937 - val_loss: 1.9951 - val_acc: 0.3692\n",
      "Epoch 33/50\n",
      "1039/1039 [==============================] - 0s 156us/sample - loss: 0.1036 - acc: 0.4947 - val_loss: 1.9995 - val_acc: 0.3731\n",
      "Epoch 34/50\n",
      "1039/1039 [==============================] - 0s 159us/sample - loss: 0.1006 - acc: 0.4937 - val_loss: 2.0115 - val_acc: 0.3692\n",
      "Epoch 35/50\n",
      "1039/1039 [==============================] - 0s 155us/sample - loss: 0.0971 - acc: 0.4937 - val_loss: 2.0115 - val_acc: 0.3731\n",
      "Epoch 36/50\n",
      "1039/1039 [==============================] - 0s 156us/sample - loss: 0.0989 - acc: 0.4947 - val_loss: 2.0210 - val_acc: 0.3731\n",
      "Epoch 37/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 0.0969 - acc: 0.4947 - val_loss: 2.0156 - val_acc: 0.3692\n",
      "Epoch 38/50\n",
      "1039/1039 [==============================] - 0s 176us/sample - loss: 0.0950 - acc: 0.4947 - val_loss: 2.0288 - val_acc: 0.3654\n",
      "Epoch 39/50\n",
      "1039/1039 [==============================] - 0s 168us/sample - loss: 0.0946 - acc: 0.4937 - val_loss: 2.0303 - val_acc: 0.3654\n",
      "Epoch 40/50\n",
      "1039/1039 [==============================] - 0s 163us/sample - loss: 0.0954 - acc: 0.4947 - val_loss: 2.0226 - val_acc: 0.3731\n",
      "Epoch 41/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 0.0920 - acc: 0.4947 - val_loss: 2.0357 - val_acc: 0.3731\n",
      "Epoch 42/50\n",
      "1039/1039 [==============================] - 0s 169us/sample - loss: 0.0936 - acc: 0.4947 - val_loss: 2.0362 - val_acc: 0.3654\n",
      "Epoch 43/50\n",
      "1039/1039 [==============================] - 0s 166us/sample - loss: 0.0926 - acc: 0.4947 - val_loss: 2.0402 - val_acc: 0.3731\n",
      "Epoch 44/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 0.0953 - acc: 0.4966 - val_loss: 2.0385 - val_acc: 0.3731\n",
      "Epoch 45/50\n",
      "1039/1039 [==============================] - 0s 161us/sample - loss: 0.0941 - acc: 0.4957 - val_loss: 2.0486 - val_acc: 0.3731\n",
      "Epoch 46/50\n",
      "1039/1039 [==============================] - 0s 174us/sample - loss: 0.0984 - acc: 0.4957 - val_loss: 2.0377 - val_acc: 0.3731\n",
      "Epoch 47/50\n",
      "1039/1039 [==============================] - 0s 172us/sample - loss: 0.1002 - acc: 0.4957 - val_loss: 2.0478 - val_acc: 0.3692\n",
      "Epoch 48/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 0.0956 - acc: 0.4957 - val_loss: 2.0652 - val_acc: 0.3654\n",
      "Epoch 49/50\n",
      "1039/1039 [==============================] - 0s 160us/sample - loss: 0.0943 - acc: 0.4976 - val_loss: 2.0595 - val_acc: 0.3731\n",
      "Epoch 50/50\n",
      "1039/1039 [==============================] - 0s 160us/sample - loss: 0.0938 - acc: 0.4966 - val_loss: 2.0494 - val_acc: 0.3731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2177df7a488>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 =Sequential()\n",
    "model3.add(Embedding(max_words,30))\n",
    "model3.add(LSTM(40))\n",
    "model3.add(Dense(1, activation='relu'))\n",
    "model3.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy']) \n",
    "model3.fit(X_train, y_train, epochs=50, batch_size=30, verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 50us/sample - loss: 2.8339 - acc: 0.2350\n",
      "[2.8339485549926757, 0.235]\n"
     ]
    }
   ],
   "source": [
    "print(model3.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINED_CLASSIFIER_PATH = \"dual_encoder_lstm_classifier.h5\" # 모델이름\n",
    "\n",
    "#model3.save(TRAINED_CLASSIFIER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model3.save('lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1단계 : 기분을 대입\n",
    "feel = '아침엔 악몽을 꾸어서 짜증났지만 오늘 문제도 잘 풀리고 컨디션이 좋아져서 상쾌한 오전이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2단계 : 대입한 기분 전처리( 토큰화, 벡터화 등) \n",
    "def Preprocess(feel):\n",
    "    X = okt.morphs(feel, stem=True, norm=True)\n",
    "    X = [word for word in X if not word in stopwords]\n",
    "    \n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=max_len)\n",
    "    return X      # X는 예측할 데이터 전처리한 결과    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3단계 : 모델에 대입 \n",
    "predict = model3.predict(Preprocess(feel))   # 전처리한 결과 모델에 넣었다\n",
    "# print(predict)  # 예측결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://codepractice.tistory.com/71\n",
    "def Delete(predict):\n",
    "    predict = predict.reshape(-1).astype('int')\n",
    "    predict = np.around(predict)\n",
    "    from collections import Counter    # 최빈값\n",
    "    list = []\n",
    "    for i in range(len(predict)):\n",
    "        if predict[i]!=2 and predict[i]!=5:   # 2, 5제거(중성단어)\n",
    "            a=list.append(predict[i])       \n",
    "            \n",
    "    if Counter(list).most_common(1)[0][0] ==0:print('아마 당신의 기분은 angry 로 추정됩니다')\n",
    "    if Counter(list).most_common(1)[0][0] ==1:print('아마 당신의 기분은 happy 로 추정됩니다')\n",
    "    if Counter(list).most_common(1)[0][0] ==3:print('아마 당신의 기분은 sad 로 추정됩니다')\n",
    "    if Counter(list).most_common(1)[0][0] ==4:print('아마 당신의 기분은 afraid 로 추정됩니다')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아마 당신의 기분은 happy 로 추정됩니다\n"
     ]
    }
   ],
   "source": [
    "Delete(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 신경망의 딥러닝을 이용한 EEG 신호로부터 공포감정 인식시트템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://sclab.yonsei.ac.kr/publications/Papers/KC/2016_KCC_fall_WGH.pdf\n",
    "https://wikidocs.net/49071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존의 방식 (MLP) : 다층 퍼셉트론으로 FFNN 의 기본적 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299, 8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층 퍼셉트론 모델(MLP)\n",
    "num_classes = 5\n",
    "def fit_and_evaluate(X_train,y_train, X_test, y_test):\n",
    "    model4 = Sequential()\n",
    "    model4.add(Dense(256, input_shape=(-1,-1,-1), activation='relu'))\n",
    "    model4.add(Dropout(0.5))\n",
    "    model4.add(Dense(128, activation='relu'))\n",
    "    model4.add(Dropout(0.5))\n",
    "    model4.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model4.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model4.fit(X_train, y_train, batch_size=128, epochs=50, verbose=1, validation_split=0.1)\n",
    "    score = model4.evaluate(X_test, y_test, batch_size=128, verbose=0)\n",
    "    return score[1]\n",
    "\n",
    "# 4개의 층으로 구성되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error converting shape to a TensorShape: Dimension -1 must be >= 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m   1203\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    715\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dimension %d must be >= 0\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimension -1 must be >= 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-6efa3c93b83a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# MLP 보다는 LSTM 이 더 높다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-b14f2a5910e0>\u001b[0m in \u001b[0;36mfit_and_evaluate\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfit_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmodel4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    168\u001b[0m           \u001b[1;31m# Instantiate an input layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m           x = input_layer.Input(\n\u001b[1;32m--> 170\u001b[1;33m               batch_shape=batch_shape, dtype=dtype, name=layer.name + '_input')\n\u001b[0m\u001b[0;32m    171\u001b[0m           \u001b[1;31m# This will build the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m           \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_size, name, dtype, sparse, tensor, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         input_tensor=tensor)\n\u001b[0m\u001b[0;32m    249\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     input_layer = InputLayer(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m               \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m               \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m               name=self.name)\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[0;32m    996\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   2141\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   2142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2143\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   6257\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6258\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6259\u001b[1;33m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6260\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   6261\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    149\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n\u001b[1;32m--> 151\u001b[1;33m                                                                     e))\n\u001b[0m\u001b[0;32m    152\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error converting shape to a TensorShape: Dimension -1 must be >= 0."
     ]
    }
   ],
   "source": [
    "fit_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "# MLP 보다는 LSTM 이 더 높다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1169 samples, validate on 130 samples\n",
      "Epoch 1/5\n",
      "1169/1169 [==============================] - 0s 143us/sample - loss: 73.6738 - acc: 0.2335 - val_loss: 41.3735 - val_acc: 0.3154\n",
      "Epoch 2/5\n",
      "1169/1169 [==============================] - 0s 20us/sample - loss: 54.6838 - acc: 0.2216 - val_loss: 12.6064 - val_acc: 0.3000\n",
      "Epoch 3/5\n",
      "1169/1169 [==============================] - 0s 21us/sample - loss: 37.0925 - acc: 0.2558 - val_loss: 15.6365 - val_acc: 0.2308\n",
      "Epoch 4/5\n",
      "1169/1169 [==============================] - 0s 22us/sample - loss: 27.9200 - acc: 0.2472 - val_loss: 8.1466 - val_acc: 0.1923\n",
      "Epoch 5/5\n",
      "1169/1169 [==============================] - 0s 22us/sample - loss: 20.3700 - acc: 0.2352 - val_loss: 4.0270 - val_acc: 0.1923\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(256, input_shape=(max_len,), activation='relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(128, activation='relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "model4.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.1)\n",
    "score = model4.evaluate(X_test, y_test, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4575142 , 0.19024019, 0.00161356, 0.16755472, 0.1830773 ],\n",
       "       [0.20553158, 0.20169427, 0.18605295, 0.20567496, 0.20104623],\n",
       "       [0.21563472, 0.20600607, 0.16412838, 0.20911218, 0.20511867],\n",
       "       [0.20553158, 0.20169427, 0.18605295, 0.20567496, 0.20104623],\n",
       "       [0.22687425, 0.21019775, 0.14164497, 0.21225345, 0.20902966]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feel2 = '너가 아파서 마음이 아팠고 슬펐다'\n",
    "predict2=model4.predict(Preprocess(feel2))\n",
    "predict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아마 당신의 기분은 angry 로 추정됩니다\n"
     ]
    }
   ],
   "source": [
    "Delete(predict2)  #>>> MLP 보다 LSTM 이 효과적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여러개의 LSTM 셀로 이루어진 layer 로 여러층을 쌓은 심층 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Activation\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: <keras.layers.core.Activation object at 0x000002170A8D1E88>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-d60d6392fe07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m46\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    152\u001b[0m       raise TypeError('The added layer must be '\n\u001b[0;32m    153\u001b[0m                       \u001b[1;34m'an instance of class Layer. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                       'Found: ' + str(layer))\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: <keras.layers.core.Activation object at 0x000002170A8D1E88>"
     ]
    }
   ],
   "source": [
    "model5 = tensorflow.keras.Sequential()\n",
    "model5.add(InputLayer(input_shape=(1299,8)))\n",
    "model5.add(LSTM(20, return_sequences = True))\n",
    "model5.add(LSTM(20, return_sequences = True))\n",
    "model5.add(LSTM(20, return_sequences = True))\n",
    "model5.add(LSTM(20, return_sequences = False))\n",
    "model5.add(Dense(46))\n",
    "model5.add(Activation('relu'))\n",
    "model5.compile(loss = 'sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1039 samples, validate on 260 samples\n",
      "Epoch 1/50\n",
      "1039/1039 [==============================] - 1s 675us/sample - loss: 4.5630 - acc: 0.2570 - val_loss: 2.2076 - val_acc: 0.3269\n",
      "Epoch 2/50\n",
      "1039/1039 [==============================] - 0s 193us/sample - loss: 2.4133 - acc: 0.2685 - val_loss: 2.0990 - val_acc: 0.3269\n",
      "Epoch 3/50\n",
      "1039/1039 [==============================] - 0s 163us/sample - loss: 2.2575 - acc: 0.2685 - val_loss: 2.0480 - val_acc: 0.3269\n",
      "Epoch 4/50\n",
      "1039/1039 [==============================] - 0s 163us/sample - loss: 2.0950 - acc: 0.2685 - val_loss: 1.9578 - val_acc: 0.3269\n",
      "Epoch 5/50\n",
      "1039/1039 [==============================] - 0s 167us/sample - loss: 1.7392 - acc: 0.2685 - val_loss: 1.8160 - val_acc: 0.3269\n",
      "Epoch 6/50\n",
      "1039/1039 [==============================] - 0s 164us/sample - loss: 1.2660 - acc: 0.2724 - val_loss: 1.7188 - val_acc: 0.3269\n",
      "Epoch 7/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.8628 - acc: 0.2849 - val_loss: 1.7389 - val_acc: 0.3115\n",
      "Epoch 8/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.5721 - acc: 0.3138 - val_loss: 1.7028 - val_acc: 0.3385\n",
      "Epoch 9/50\n",
      "1039/1039 [==============================] - 0s 159us/sample - loss: 0.4111 - acc: 0.3484 - val_loss: 1.7206 - val_acc: 0.3538\n",
      "Epoch 10/50\n",
      "1039/1039 [==============================] - 0s 161us/sample - loss: 0.3061 - acc: 0.3705 - val_loss: 1.7420 - val_acc: 0.3692\n",
      "Epoch 11/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.2384 - acc: 0.3927 - val_loss: 1.7752 - val_acc: 0.3654\n",
      "Epoch 12/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.1982 - acc: 0.4485 - val_loss: 1.7935 - val_acc: 0.3769\n",
      "Epoch 13/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.1735 - acc: 0.4716 - val_loss: 1.8062 - val_acc: 0.3808\n",
      "Epoch 14/50\n",
      "1039/1039 [==============================] - 0s 175us/sample - loss: 0.1599 - acc: 0.4764 - val_loss: 1.8753 - val_acc: 0.3692\n",
      "Epoch 15/50\n",
      "1039/1039 [==============================] - 0s 170us/sample - loss: 0.1567 - acc: 0.4832 - val_loss: 1.8922 - val_acc: 0.3692\n",
      "Epoch 16/50\n",
      "1039/1039 [==============================] - 0s 166us/sample - loss: 0.1416 - acc: 0.4860 - val_loss: 1.8856 - val_acc: 0.3692\n",
      "Epoch 17/50\n",
      "1039/1039 [==============================] - 0s 164us/sample - loss: 0.1305 - acc: 0.4851 - val_loss: 1.8958 - val_acc: 0.3731\n",
      "Epoch 18/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 0.1235 - acc: 0.4860 - val_loss: 1.9106 - val_acc: 0.3692\n",
      "Epoch 19/50\n",
      "1039/1039 [==============================] - 0s 161us/sample - loss: 0.1178 - acc: 0.4870 - val_loss: 1.9241 - val_acc: 0.3654\n",
      "Epoch 20/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.1176 - acc: 0.4880 - val_loss: 1.9358 - val_acc: 0.3654\n",
      "Epoch 21/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.1190 - acc: 0.4870 - val_loss: 1.9347 - val_acc: 0.3615\n",
      "Epoch 22/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.1141 - acc: 0.4889 - val_loss: 1.9545 - val_acc: 0.3654\n",
      "Epoch 23/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 0.1128 - acc: 0.4889 - val_loss: 1.9590 - val_acc: 0.3654\n",
      "Epoch 24/50\n",
      "1039/1039 [==============================] - 0s 167us/sample - loss: 0.1079 - acc: 0.4889 - val_loss: 1.9637 - val_acc: 0.3654\n",
      "Epoch 25/50\n",
      "1039/1039 [==============================] - 0s 167us/sample - loss: 0.1051 - acc: 0.4889 - val_loss: 1.9830 - val_acc: 0.3654\n",
      "Epoch 26/50\n",
      "1039/1039 [==============================] - 0s 166us/sample - loss: 0.1078 - acc: 0.4899 - val_loss: 1.9872 - val_acc: 0.3654\n",
      "Epoch 27/50\n",
      "1039/1039 [==============================] - 0s 166us/sample - loss: 0.1135 - acc: 0.4899 - val_loss: 1.9822 - val_acc: 0.3654\n",
      "Epoch 28/50\n",
      "1039/1039 [==============================] - 0s 175us/sample - loss: 0.1227 - acc: 0.4899 - val_loss: 2.0061 - val_acc: 0.3654\n",
      "Epoch 29/50\n",
      "1039/1039 [==============================] - 0s 170us/sample - loss: 0.1120 - acc: 0.4889 - val_loss: 1.9966 - val_acc: 0.3654\n",
      "Epoch 30/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 0.1034 - acc: 0.4909 - val_loss: 1.9981 - val_acc: 0.3654\n",
      "Epoch 31/50\n",
      "1039/1039 [==============================] - 0s 168us/sample - loss: 0.1069 - acc: 0.4928 - val_loss: 1.9978 - val_acc: 0.3654\n",
      "Epoch 32/50\n",
      "1039/1039 [==============================] - 0s 179us/sample - loss: 0.1055 - acc: 0.4909 - val_loss: 2.0045 - val_acc: 0.3654\n",
      "Epoch 33/50\n",
      "1039/1039 [==============================] - 0s 168us/sample - loss: 0.1043 - acc: 0.4937 - val_loss: 2.0097 - val_acc: 0.3654\n",
      "Epoch 34/50\n",
      "1039/1039 [==============================] - 0s 163us/sample - loss: 0.1027 - acc: 0.4937 - val_loss: 2.0027 - val_acc: 0.3654\n",
      "Epoch 35/50\n",
      "1039/1039 [==============================] - 0s 163us/sample - loss: 0.1034 - acc: 0.4928 - val_loss: 2.0125 - val_acc: 0.3654\n",
      "Epoch 36/50\n",
      "1039/1039 [==============================] - 0s 159us/sample - loss: 0.1002 - acc: 0.4947 - val_loss: 2.0285 - val_acc: 0.3654\n",
      "Epoch 37/50\n",
      "1039/1039 [==============================] - 0s 160us/sample - loss: 0.1027 - acc: 0.4928 - val_loss: 2.0244 - val_acc: 0.3654\n",
      "Epoch 38/50\n",
      "1039/1039 [==============================] - 0s 163us/sample - loss: 0.0972 - acc: 0.4937 - val_loss: 2.0311 - val_acc: 0.3654\n",
      "Epoch 39/50\n",
      "1039/1039 [==============================] - 0s 162us/sample - loss: 0.0985 - acc: 0.4947 - val_loss: 2.0096 - val_acc: 0.3654\n",
      "Epoch 40/50\n",
      "1039/1039 [==============================] - 0s 164us/sample - loss: 0.0992 - acc: 0.4928 - val_loss: 2.0362 - val_acc: 0.3654\n",
      "Epoch 41/50\n",
      "1039/1039 [==============================] - 0s 170us/sample - loss: 0.1010 - acc: 0.4928 - val_loss: 2.0286 - val_acc: 0.3615\n",
      "Epoch 42/50\n",
      "1039/1039 [==============================] - 0s 172us/sample - loss: 0.1018 - acc: 0.4947 - val_loss: 2.0461 - val_acc: 0.3615\n",
      "Epoch 43/50\n",
      "1039/1039 [==============================] - 0s 166us/sample - loss: 0.1084 - acc: 0.4918 - val_loss: 2.0387 - val_acc: 0.3615\n",
      "Epoch 44/50\n",
      "1039/1039 [==============================] - 0s 164us/sample - loss: 0.1132 - acc: 0.4937 - val_loss: 2.0238 - val_acc: 0.3615\n",
      "Epoch 45/50\n",
      "1039/1039 [==============================] - 0s 169us/sample - loss: 0.1018 - acc: 0.4937 - val_loss: 2.0740 - val_acc: 0.3615\n",
      "Epoch 46/50\n",
      "1039/1039 [==============================] - 0s 165us/sample - loss: 0.0983 - acc: 0.4947 - val_loss: 2.0317 - val_acc: 0.3615\n",
      "Epoch 47/50\n",
      "1039/1039 [==============================] - 0s 160us/sample - loss: 0.0965 - acc: 0.4957 - val_loss: 2.0520 - val_acc: 0.3615\n",
      "Epoch 48/50\n",
      "1039/1039 [==============================] - 0s 166us/sample - loss: 0.0932 - acc: 0.4957 - val_loss: 2.0473 - val_acc: 0.3615\n",
      "Epoch 49/50\n",
      "1039/1039 [==============================] - 0s 172us/sample - loss: 0.0952 - acc: 0.4957 - val_loss: 2.0672 - val_acc: 0.3615\n",
      "Epoch 50/50\n",
      "1039/1039 [==============================] - 0s 177us/sample - loss: 0.0922 - acc: 0.4947 - val_loss: 2.0430 - val_acc: 0.3577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2170546b588>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 =tensorflow.keras.Sequential()\n",
    "model3.add(Embedding(max_words,30))\n",
    "model3.add(LSTM(40))\n",
    "model3.add(Dense(1))`\n",
    "model3.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy']) \n",
    "\n",
    "model3.fit(X_train, y_train, epochs=50, batch_size=30, verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
